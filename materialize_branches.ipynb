{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61be24e-1347-46cd-b564-e538fcb8c852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! xrdcp -f root://xrootd-local.unl.edu//store/user/IDAP/zstd_files.json zstd_files.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce7844-00b3-4129-ad5a-1aa64a46bd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "from dask.distributed import Client, performance_report\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "    \n",
    "executor = \"dask\"   # \"dask\" or \"taskvine\"\n",
    "\n",
    "# import utils\n",
    "# utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "    \n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891e515-6bef-4ac3-a72f-9fec2b13c25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler_options = {}\n",
    "\n",
    "# for coffea-casa\n",
    "if executor == \"taskvine\":\n",
    "    from ndcctools.taskvine import DaskVine, Task\n",
    "    \n",
    "    \n",
    "    manager = DaskVine(port=8788, ssl=True, name=f\"{os.environ.get('USER', 'noname')}-coffea-casa\")\n",
    "    manager.disable_peer_transfers()  # disable xfers between workers until figuring out docker routing\n",
    "\n",
    "    extra_files = {}\n",
    "    env_vars = {}\n",
    "    \n",
    "    token_path = \"/etc/cmsaf-secrets/access_token\"\n",
    "    if Path(token_path).is_file():\n",
    "        local_token = manager.declare_file(token_path, cache=True)\n",
    "\n",
    "        # Manually change mode of access token, as it needs to be 0600\n",
    "        # Alternatively, we could copy it to execution directory and change the mode there.\n",
    "        do_chmod = Task(\"chmod 0600 access_token\")\n",
    "        do_chmod.add_input(local_token, \"access_token\")\n",
    "        token_file = manager.declare_minitask(do_chmod, \"access_token\", cache=True, peer_transfer=True)\n",
    "        extra_files = {token_file: \"access_token\"}\n",
    "        env_vars = {\"BEARER_TOKEN_FILE\": \"access_token\"}\n",
    "\n",
    "    vine_scheduler = partial(manager.get,\n",
    "                             resources={\"cores\": 1, \"disk\": 2000},  #  max 1 core, 5GB of disk per task\n",
    "                             extra_files=extra_files,\n",
    "                             env_vars=env_vars,\n",
    "                             submit_per_cycle=1000,\n",
    "                             #  resources_mode=None,   # set to \"fixed\" to kill tasks on resources\n",
    "                            )\n",
    "    # change default scheduler\n",
    "    scheduler_options['scheduler'] = vine_scheduler\n",
    "else:\n",
    "    # by default use dask   \n",
    "    # local: single thread, single worker\n",
    "    from dask.distributed import LocalCluster, Client, progress\n",
    "    \n",
    "    # cluster = LocalCluster(n_workers=1, processes=False, threads_per_worker=1)\n",
    "    # client = Client(cluster)\n",
    "    client = Client(\"tls://localhost:8786\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def task(events):\n",
    "    # track number of events\n",
    "    num_events = ak.num(events, axis=0)\n",
    "    \n",
    "    # hit all the other branches, just derive integers from them that will be aggregated to avoid memory issues\n",
    "    _counter = 0\n",
    "    _counter += ak.count_nonzero(events.GenPart.pt)\n",
    "    _counter += ak.count_nonzero(events.GenPart.eta)\n",
    "    _counter += ak.count_nonzero(events.GenPart.phi)\n",
    "    _counter += ak.count_nonzero(events.CorrT1METJet.phi)\n",
    "    _counter += ak.count_nonzero(events.GenJet.pt)\n",
    "    _counter += ak.count_nonzero(events.CorrT1METJet.eta)\n",
    "    _counter += ak.count_nonzero(events.SoftActivityJet.pt)\n",
    "    _counter += ak.count_nonzero(events.Jet.eta)\n",
    "    _counter += ak.count_nonzero(events.Jet.phi)\n",
    "    _counter += ak.count_nonzero(events.SoftActivityJet.eta)\n",
    "    _counter += ak.count_nonzero(events.SoftActivityJet.phi)\n",
    "    _counter += ak.count_nonzero(events.LHEPart.eta)\n",
    "    _counter += ak.count_nonzero(events.LHEPart.phi)\n",
    "    _counter += ak.count_nonzero(events.CorrT1METJet.rawPt)\n",
    "    _counter += ak.count_nonzero(events.Jet.btagDeepFlavB)\n",
    "    _counter += ak.count_nonzero(events.GenJet.eta)\n",
    "    _counter += ak.count_nonzero(events.GenPart.mass)\n",
    "    _counter += ak.count_nonzero(events.GenJet.phi)\n",
    "    _counter += ak.count_nonzero(events.Jet.puIdDisc)\n",
    "    _counter += ak.count_nonzero(events.CorrT1METJet.muonSubtrFactor)\n",
    "    _counter += ak.count_nonzero(events.Jet.btagDeepFlavCvL)\n",
    "    _counter += ak.count_nonzero(events.LHEPart.mass)\n",
    "    _counter += ak.count_nonzero(events.LHEPart.pt)\n",
    "    _counter += ak.count_nonzero(events.Jet.btagDeepFlavQG)\n",
    "    _counter += ak.count_nonzero(events.Jet.mass)\n",
    "    _counter += ak.count_nonzero(events.Jet.pt)\n",
    "    _counter += ak.count_nonzero(events.GenPart.pdgId)\n",
    "    _counter += ak.count_nonzero(events.Jet.btagDeepFlavCvB)\n",
    "    _counter += ak.count_nonzero(events.Jet.cRegCorr)\n",
    "    _counter += ak.count_nonzero(events.LHEPart.incomingpz)\n",
    "\n",
    "    return {\"nevts\": num_events, \"_counter\": _counter}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f24e12-3fd8-481c-8318-7f9277f72c50",
   "metadata": {},
   "source": [
    "just run over a local DY file here as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f1240-a754-4dd1-b861-1bfac65ec288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fileset = {\"DY\": {\"files\": {\"0263846E-B57D-7E48-A80F-458F8445E6C6.root\": \"Events\"}}}\n",
    "# fileset\n",
    "import json\n",
    "fname = \"zstd_files.json\"\n",
    "fileset = {}\n",
    "with open(fname,'r') as fp:\n",
    "    for i,(dataset_name,file_list) in enumerate(json.load(fp).items()):\n",
    "        if i != 0:\n",
    "            continue\n",
    "        fileset[dataset_name] = {\"files\": {}}\n",
    "        for j,dataset_fpath in enumerate(file_list):\n",
    "            if j != 1:\n",
    "                continue\n",
    "            # xrd_fpath = f\"root://xrootd-local.unl.edu/{dataset_fpath}\"\n",
    "            xrd_fpath = f\"root://xcache/{dataset_fpath}\"\n",
    "            # fileset[dataset_name][\"files\"][xrd_fpath] = {\"object_path\": \"Events\"}\n",
    "            print(f\"! xrdcp -f {xrd_fpath} /dev/null\")\n",
    "            fileset[dataset_name][\"files\"][xrd_fpath] = \"Events\"\n",
    "            break\n",
    "        break\n",
    "fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset, step_size=500_000,uproot_options={\"allow_read_errors_with_report\": True},**scheduler_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(task, samples, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "execute task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "if executor == \"taskvine\":\n",
    "    ((out, report),) = dask.compute(tasks, **scheduler_options)\n",
    "else:\n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        ((out, report),) = dask.compute(tasks, **scheduler_options)  # feels strange that this is a tuple-of-tuple\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"total time spent in uproot reading data: {ak.sum([v['duration'] for v in report.values()]):.2f} s\")\n",
    "print(f\"wall time: {t1-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd336cbb-22a1-45f8-9f85-e18326065c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f42084-1139-48e6-95c3-b7f9217033a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_rate = out[\"DY\"][\"nevts\"] / (t1-t0)\n",
    "print(f\"event rate: {event_rate / 1_000:.2f} kHz\")\n",
    "\n",
    "# need uproot>=5.3.2 to get these useful performance stats\n",
    "read_MB = ak.sum([v['performance_counters']['num_requested_bytes'] for v in report.values()] / 1_000**2\n",
    "rate_Mbs = read_MB / (t1-t0)\n",
    "print(f\" - read {read_MB:.2f} MB in {t1-t0:.2f} s -> {rate_Mbs:.2f} MBps (need to scale by x{200/8/rate_Mbs*1000:.0f} to reach 200 Gbps)\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
